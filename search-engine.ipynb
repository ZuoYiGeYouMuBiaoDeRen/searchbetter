{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # reading harvardx data from the csv\n",
    "# # still a work in progress since their dataset is malformatted\n",
    "# import csv\n",
    "\n",
    "# # load data\n",
    "# csvfile_path = '../datasets/corpus_HarvardX_LatestCourses_based_on_2016-10-18.csv'\n",
    "\n",
    "# with open(csvfile_path, 'r') as csvfile:\n",
    "#     reader = csv.DictReader(csvfile)\n",
    "#     print reader.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'slug': u'developing-android-apps--ud853', 'title': u'Developing Android Apps'}, {'slug': u'new-android-fundamentals--ud851', 'title': u'New Android Fundamentals'}, {'slug': u'android-for-beginners--ud834', 'title': u'Android for Beginners'}, {'slug': u'android-for-beginners--ud834', 'title': u'Android for Beginners'}, {'slug': u'android-tv-and-google-cast-development--ud875B', 'title': u'Android TV and Google Cast Development'}, {'slug': u'gradle-for-android-and-java--ud867', 'title': u'Gradle for Android and Java'}, {'slug': u'android-wear-development--ud875A', 'title': u'Android Wear Development'}, {'slug': u'material-design-for-android-developers--ud862', 'title': u'Material Design for Android Developers'}, {'slug': u'android-basics-user-input--ud836', 'title': u'Android Basics: User Input'}, {'slug': u'android-basics-user-input--ud836', 'title': u'Android Basics: User Input'}]\n"
     ]
    }
   ],
   "source": [
    "import search\n",
    "reload(search)\n",
    "\n",
    "# reopen index, but reload it instead of creating it afresh\n",
    "udacity_se = search.UdacitySearchEngine(create=False)\n",
    "    \n",
    "# dictionary of results\n",
    "results = udacity_se.search(\"android\")\n",
    "print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'[[Vishnu]]', 0.4175434112548828), (u'rulers]]', 0.4047089219093323), (u'Hillary', 0.3996689021587372), (u'International_Conference', 0.39594656229019165), (u'[[Children', 0.3944295048713684), (u'understandable', 0.3920005261898041), (u'[[Harley', 0.38948845863342285), (u'Protocol&lt;br&gt;', 0.3832295536994934), (u'Lovecraft:', 0.3831127882003784), (u'align=left', 0.38207781314849854)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.models as models\n",
    "import gensim.models.word2vec as word2vec\n",
    "\n",
    "# prepare corpus and ngram reader\n",
    "# create\n",
    "# corpus = word2vec.Text8Corpus('datasets/enwik8')\n",
    "# ngram_phrases= models.Phrases(corpus)\n",
    "# ngram = models.phrases.Phraser(ngram_phrases)\n",
    "# ngram.save('models/phraser/phraser')\n",
    "# or just load from file\n",
    "ngram = models.phrases.Phraser.load('models/phraser/phraser')\n",
    "\n",
    "# load model\n",
    "# either or\n",
    "# model = word2vec.Word2Vec(ngram[corpus], workers=8)\n",
    "# model.save('models/word2vec/word2vec')\n",
    "model = word2vec.Word2Vec.load('models/word2vec/word2vec')\n",
    "\n",
    "# test model\n",
    "# BUG: this doesn't work that well :(\n",
    "# it worked pretty well before i put in ngrams\n",
    "print model.most_similar(positive=['autism'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CS1 maint: Unfit url', 'Cognitive biases', 'Cognitive inertia', 'Critical thinking', 'Design of experiments', 'Error', 'Featured articles', 'Ignorance', 'Inductive fallacies', 'Logical fallacies']\n",
      "['1879 births', '1955 deaths', '20th-century American engineers', '20th-century American writers', '20th-century German writers', '20th-century Jews', '20th-century physicists', 'AC with 19 elements', 'Activists from New Jersey', 'Albert Einstein']\n",
      "['Biochemistry', 'Biotechnology', 'CS1 maint: Uses editors parameter', 'Molecular biology', 'Pages using ISBN magic links', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers']\n"
     ]
    }
   ],
   "source": [
    "# wikipedia rewriting\n",
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "class WikipediaRewriter:\n",
    "    WIKI_BASE = 'https://en.wikipedia.org/w/api.php?format=xml&action=query&prop=categories&titles='\n",
    "    \n",
    "    def clean_category(self, x):\n",
    "        return x.replace('Category:','')\n",
    "    \n",
    "    def rewrite(self, term):\n",
    "        \"\"\"\n",
    "        Given a base term, returns a list of related terms based on the Wikipedia\n",
    "        category API.\n",
    "        \"\"\"\n",
    "        api = self.WIKI_BASE + term\n",
    "        r = requests.get(api)\n",
    "        tree = etree.fromstring(r.text)\n",
    "        # TODO: ignore wikipedia internal categories\n",
    "        # https://en.wikipedia.org/wiki/Category:Tracking_categories\n",
    "        return [self.clean_category(x.get('title')) for x in tree.findall('.//cl')]\n",
    "\n",
    "    \n",
    "test_terms = [\n",
    "    'confirmation bias',\n",
    "    'Albert Einstein',\n",
    "    'biochemistry',\n",
    "]\n",
    "\n",
    "rewriter = WikipediaRewriter()\n",
    "\n",
    "for term in test_terms:\n",
    "    print rewriter.rewrite(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('neural networks', 2, 2)\n",
      "('swift', 9, 9)\n",
      "('venture capital', 1, 1)\n",
      "('object-oriented programming', 6, 6)\n",
      "('deep learning', 1, 1)\n",
      "('macOS', 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# compare query rewriting to the normal results \n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "searches = [\n",
    "    \"neural networks\",\n",
    "    \"swift\",\n",
    "    \"venture capital\",\n",
    "    \"object-oriented programming\",\n",
    "    \"deep learning\",\n",
    "    \"macOS\"\n",
    "]\n",
    "\n",
    "for query in searches:\n",
    "    # normal results\n",
    "    num_plain_results = len(udacity_se.search(query))\n",
    "    \n",
    "    # with wikipedia rewriting\n",
    "    wiki_rewritten_queries = rewriter.rewrite(query) + [query]\n",
    "    wiki_results = [udacity_se.search(q) for q in wiki_rewritten_queries]\n",
    "    # flatten them\n",
    "    flattened_wiki_results = flatten(wiki_results)\n",
    "    num_wiki_results = len(flattened_wiki_results)\n",
    "    \n",
    "    print (query, num_plain_results, num_wiki_results)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
