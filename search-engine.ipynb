{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'slug': u'grand-central-dispatch-gcd--ud576', 'title': u'Grand Central Dispatch (GCD)'}, {'slug': u'intro-to-ios-app-development-with-swift--ud585', 'title': u'Intro to iOS App Development with Swift'}, {'slug': u'swift-for-beginners--ud1022', 'title': u'Swift for Beginners'}, {'slug': u'server-side-swift--ud1031', 'title': u'Server-Side Swift'}, {'slug': u'learn-swift-programming-syntax--ud902', 'title': u'Learn Swift Programming Syntax'}, {'slug': u'ios-persistence-and-core-data--ud325', 'title': u'iOS Persistence and Core Data'}]\n"
     ]
    }
   ],
   "source": [
    "import search\n",
    "reload(search)\n",
    "\n",
    "# reopen index, but reload it instead of creating it afresh\n",
    "udacity_se = search.UdacitySearchEngine(create=False)\n",
    "    \n",
    "# dictionary of results\n",
    "results = udacity_se.search(\"apple\")\n",
    "print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'course_id': u'course-v1:HarvardX+GSE4x+2T2016', 'display_name': u'Kathleen Hoover-Dempsey Bio'}, {'course_id': u'course-v1:HarvardX+GSE4x+2T2016', 'display_name': u'Additional Resources'}, {'course_id': u'course-v1:HarvardX+GSE4x+2T2016', 'display_name': u'Watch: Parental Motivation for Engagement'}, {'course_id': u'course-v1:HarvardX+GSE4x+2T2016', 'display_name': u'Additional Resources'}, {'course_id': u'course-v1:HarvardX+PH201x+2013_SOND', 'display_name': u'Meet the Teaching Assistants: Felicia Browne'}, {'course_id': u'course-v1:HarvardX+GSE2x+1T2016', 'display_name': u'Additional Resources'}, {'course_id': u'course-v1:HarvardX+GSE2x+1T2016', 'display_name': u'Intro to Neuroeducation'}, {'course_id': u'course-v1:HarvardX+1368.2x+2T2016', 'display_name': u'Glossary'}, {'course_id': u'course-v1:HarvardX+GSE4x+2T2016', 'display_name': u'Explore the Research'}, {'course_id': u'course-v1:HarvardX+AmPoX.6+2T2016', 'display_name': u''}]\n"
     ]
    }
   ],
   "source": [
    "import search\n",
    "reload(search)\n",
    "\n",
    "# reopen index, but reload it instead of creating it afresh\n",
    "se = search.HarvardXSearchEngine(create=False)\n",
    "    \n",
    "# dictionary of results\n",
    "results = se.search(\"Psychology\")\n",
    "print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'[[Vishnu]]', 0.4175434112548828), (u'rulers]]', 0.4047089219093323), (u'Hillary', 0.3996689021587372), (u'International_Conference', 0.39594656229019165), (u'[[Children', 0.3944295048713684), (u'understandable', 0.3920005261898041), (u'[[Harley', 0.38948845863342285), (u'Protocol&lt;br&gt;', 0.3832295536994934), (u'Lovecraft:', 0.3831127882003784), (u'align=left', 0.38207781314849854)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.models as models\n",
    "import gensim.models.word2vec as word2vec\n",
    "\n",
    "# TODO remove ngrams lol\n",
    "# prepare corpus and ngram reader\n",
    "# create\n",
    "# corpus = word2vec.Text8Corpus('datasets/enwik8')\n",
    "# ngram_phrases= models.Phrases(corpus)\n",
    "# ngram = models.phrases.Phraser(ngram_phrases)\n",
    "# ngram.save('models/phraser/phraser')\n",
    "# or just load from file\n",
    "ngram = models.phrases.Phraser.load('models/phraser/phraser')\n",
    "\n",
    "# load model\n",
    "# either or\n",
    "# model = word2vec.Word2Vec(ngram[corpus], workers=8)\n",
    "# model.save('models/word2vec/word2vec')\n",
    "model = word2vec.Word2Vec.load('models/word2vec/word2vec')\n",
    "\n",
    "# test model\n",
    "# BUG: this doesn't work that well :(\n",
    "# it worked pretty well before i put in ngrams\n",
    "print model.most_similar(positive=['autism'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('neural networks', 2, 2)\n",
      "('swift', 9, 9)\n",
      "('venture capital', 1, 1)\n",
      "('object-oriented programming', 6, 6)\n",
      "('deep learning', 1, 1)\n",
      "('macOS', 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# compare query rewriting to the normal results \n",
    "import rewriter\n",
    "reload(rewriter)\n",
    "\n",
    "import search\n",
    "reload(search)\n",
    "\n",
    "searches = [\n",
    "    \"neural networks\",\n",
    "    \"swift\",\n",
    "    \"venture capital\",\n",
    "    \"object-oriented programming\",\n",
    "    \"deep learning\",\n",
    "    \"macOS\"\n",
    "]\n",
    "\n",
    "\n",
    "udacity_se = search.UdacitySearchEngine(create=False)\n",
    "rw = rewriter.WikipediaRewriter()\n",
    "cw = rewriter.ControlRewriter()\n",
    "\n",
    "wikidacity = rewriter.RewritingSearchEngine(rw, udacity_se)\n",
    "plaindacity = rewriter.RewritingSearchEngine(cw, udacity_se)\n",
    "\n",
    "for query in searches:\n",
    "    # normal results\n",
    "    num_plain_results = len(plaindacity.search(query))\n",
    "    \n",
    "    # with wikipedia rewriting\n",
    "    num_wiki_results = len(wikidacity.search(query))\n",
    "    \n",
    "    print (query, num_plain_results, num_wiki_results)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
